<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Toy&#39;s Blogs</title>
    <link>http://keltoy.github.io/</link>
    <description>Recent content on Toy&#39;s Blogs</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&lt;a href=&#34;https://creativecommons.org/licenses/by-nc/4.0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CC BY-NC 4.0&lt;/a&gt;</copyright>
    <lastBuildDate>Thu, 27 Aug 2020 20:40:45 +0800</lastBuildDate><atom:link href="http://keltoy.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Spark_内存管理</title>
      <link>http://keltoy.github.io/posts/spark_%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/</link>
      <pubDate>Thu, 27 Aug 2020 20:40:45 +0800</pubDate>
      
      <guid>http://keltoy.github.io/posts/spark_%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/</guid>
      <description>前言 Spark是基于内存的计算引擎，就是说它高效的使用了分布式节点上的内存资源，尽可能多的使用内存，而不是将数据写入磁盘。内存管理机制就是其中的核心Spark 是基于内存进行处理，不用每次计算后写入磁盘，再取出来进行计算，这样就节省了IO时间，所有的临时数据都放在内存中。
Spark管理的内存  系统区，Spark运行自身的代码需要的空间 用户区，udf等代码需要一定的空间来执行 存储区，为了计算速度加快，Spark会将存储的数据放入内存中进行计算，存储区就是为了放入数据使用的 执行区，Spark操作数据的单元是partition，Spark在执行一些shuffle、join、sort、aggregation之类的操作，需要把partition加载到内存进行计算，会用到部分内存  总结 </description>
    </item>
    
    <item>
      <title>Kylin_And_Doris</title>
      <link>http://keltoy.github.io/posts/kylin_and_doris/</link>
      <pubDate>Mon, 03 Aug 2020 16:10:51 +0800</pubDate>
      
      <guid>http://keltoy.github.io/posts/kylin_and_doris/</guid>
      <description>[toc]
前言 Kylin 和 Doris 都是 开源OLAP 对比这两款数据库，实际上也是是 MOLAP 和 ROLAP的代表
​ MOLAP (Multidimension OLAP)，存储模式使得分区的聚合和其源数据的复本以多维结构存储在分析服务器计算机上
​ ROLAP (Relational OLAP)，存储模式使得分区的聚合存储在关系数据库的表（在分区数据源中指定）中
​ HOLAP (Hybrid OLAP)，支持所有的三种存储模式。应用存储设计向导，可以选择最适合于分区的存储模式。或者也可使用基于使用的优化向导，以便根据已发送到多维数据集的查询选择存储模式并优化聚合设计。当使用三种存储模式之一时，还可以使用显式定义的筛选来限制读入到分区内的源数据
MOLAP  概述：  这是OLAP分析的传统方式。在MOLAP中，数据存储在一个多维数据集(cube)中，存储并不是在传统的关系型数据库中，而是自定义的格式。
 优势：  卓越的性能：MOLAP cubes为了快速数据检索而构建，具有最佳的slicing dicing操作
可以执行复杂的计算：所有的计算都在创建多维数据表时预先生成。因此，复杂的计算不仅可行，而且迅速
 劣势：  它可以处理的数据量有限：因为所有的计算都是执行在构建的多维数据集上，多维数据集本身不可能包括大量的数据。当然这并不是大数据不能派生出多维数据集。事实上，这是可以的。但是在这种情况下，只有汇总的信息能够包含在多维数据集中。
需要额外的成本：多维数据集技术往往是有专利或现在并不存在在某个组织中。因此，要想采用MOLAP技术，通常是要付出额外的人力和资源成本。
ROLAP  概述：  这种方法依赖于操作存储在关系型数据库中的数据，给传统的OLAP slicing 和 dicing功能。本质上，每个slicing或dicing功能和SQL语句中&amp;quot;WHERE&amp;quot;子句的功能是一样的。
 优势：  可以处理大数据量：ROLAP技术的数据量大小就是底层关系数据库存储的大小。换句话说，ROLAP本身没有对数据量的限制。
可以利用关系型数据库所固有的功能：关系型数据库已经具备非常多的功能。ROLAP技术，由于它是建立在关系型数据库上的，因此可以使用这些功能。
 劣势：  性能可能会很慢：因为每个ROLAP包裹实际上是一个SQL查询（或多个SQL查询）关系数据库，可能会因为底层数据量很大，使得查询的时间很长。
受限于SQL的功能：因为ROLAP技术主要依赖于生成SQL语句查询关系数据库，SQL语句并不能满足所有的需求（举例来说，使用SQL很难执行复杂的计算），ROLAP技术因此受限于SQL所能做的事情。ROLAP厂商已经通过构建工具以减轻这种风险，而且允许用户自定义函数。
HOLAP  概述：  HOLAP技术试图将MOLAP和ROLAP技术的优势结合起来。总体来说，HOLAP利用了多维数据集的技术从而得到更快的性能。
当需要详细信息时，HOLAP可以从多维数据集“穿过”到底层的关系数据库。</description>
    </item>
    
    <item>
      <title>About</title>
      <link>http://keltoy.github.io/about/</link>
      <pubDate>Sun, 26 Jul 2020 11:20:44 +0800</pubDate>
      
      <guid>http://keltoy.github.io/about/</guid>
      <description>Long may the sunshine! </description>
    </item>
    
    <item>
      <title>Sequence Parquet And Avro</title>
      <link>http://keltoy.github.io/posts/sequence-parquet-and-avro/</link>
      <pubDate>Fri, 24 Jul 2020 17:54:40 +0000</pubDate>
      
      <guid>http://keltoy.github.io/posts/sequence-parquet-and-avro/</guid>
      <description>[toc]
背景 大数据常用文件格式，在hive spark 使用时都需要注意，现在在使用flink 的时候发现格式问题还挺头疼的，准备整理整理，认清楚各个文件格式是怎么一回事
文件格式 Sequence File  sequenceFile文件是Hadoop用来存储二进制形式的[Key,Value]对而设计的一种平面文件(Flat File)。 可以把SequenceFile当做是一个容器，把所有的文件打包到SequenceFile类中可以高效的对小文件进行存储和处理。 SequenceFile文件并不按照其存储的Key进行排序存储，SequenceFile的内部类Writer提供了append功能。 SequenceFile中的Key和Value可以是任意类型Writable或者是自定义Writable。 在存储结构上，SequenceFile主要由一个Header后跟多条Record组成，Header主要包含了Key classname，value classname，存储压缩算法，用户自定义元数据等信息，此外，还包含了一些同步标识，用于快速定位到记录的边界。每条Record以键值对的方式进行存储，用来表示它的字符数组可以一次解析成：记录的长度、Key的长度、Key值和value值，并且Value值的结构取决于该记录是否被压缩。  Sequence File 有3中压缩方式
 无压缩：不启用压缩，那么每个记录就由它的记录长度、键的长度，和键、值组成 记录压缩 ：和无压缩格式基本相同，不同的是值字节是用定义在头部的编码器来压缩的  块压缩：块压缩一次多个记录，因此比记录压缩更紧凑，推荐  Parquet Apache Parquet是一种能够有效存储嵌套数据的列式存储格式。
 Parquet文件由一个文件头（header），一个或多个紧随其后的文件块（block），以及一个用于结尾的文件尾（footer）构成。 Parquet文件的每个文件块负责存储一个行组，行组由列块组成，且一个列块负责存储一列数据。每个列块中的的数据以页为单位。 行组 可以理解为一个个block，这个特性让parquet是可分割的，因此可以被mapreduce split来处理 不论是行组，还是page，都有具体的统计信息，根据这些统计信息可以做很多优化 每个行组由一个个 Column chunk组成，也就是一个个列。Column chunk又细分成一个个page，每个page下就是该列的数据集合。列下面再细分page主要是为了添加索引，page容量设置的小一些可以增加索引的速度，但是设置太小也会导致过多的索引和统计数据，不仅占用空间，还会降低扫描索引的时间。 parquet可以支持嵌套的数据结构，它使用了Dremel的 Striping/Assembly 算法来实现对嵌套型数据结构的打散和重构 parquet的索引和元数据全部放在footer块  Avro avro文件格式大致如下
 header, followed by one or more file data blocks  其中，datablock又可分为
 numEntries：该datablock中的记录条数； blockSize：该datablock的大小； data：存储的数据； sync：同步位  整个avro的文件布局如下：
与Thrift 的区别：</description>
    </item>
    
    <item>
      <title>大数据框架</title>
      <link>http://keltoy.github.io/posts/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A1%86%E6%9E%B6/</link>
      <pubDate>Wed, 01 Jul 2020 10:40:25 +0000</pubDate>
      
      <guid>http://keltoy.github.io/posts/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A1%86%E6%9E%B6/</guid>
      <description>大数据框架 [toc]
背景 做数据处理不能只是出报表，有很多东西还需要去处理，比如用户画像，比如数据分析，比如推荐系统，比如数据仓库。
现在处理数据不像原来，只需要处理离线数据，这样报表也只能出T-1天的数据，很多时候，我们希望能看到实时数据。这样就会出现一些问题：
更新频率，一天、一小时更新，这样可以不使用实时处理，可能体现不了实时的意义；如果都改成流数据处理，更新频率可以到达分钟，秒级，但是数据量不全，缺少历史数据，可能不能保证准确性。因此需要设计一种架构技能满足实时处理，又要保证历史数据准确。
Lambda 框架 Lambda架构的设计为了处理大规模数据时，同时发挥流数据处理和批处理的优势。通过离线批处理提供全面、准确的数据；通过实时流处理提供低延迟的数据，达到平衡延迟、吞吐量和容错性的目的。
Lambda架构包含3层Batch Layer， Speed Layer，Serving Layer
 Batch Layer: 批处理层，对离线的历史数据进行预结算，为了下游能够快速查询想要的结果。由于批处理基于完整的历史数据集因此准确性可以得到保证。批处理可以用Hadoop/Spark/Flink等框架计算 Speed Layer: 加速处理层，处理实时的增量数据，这一层重点在于低延迟。加速层的数据不如批处理那样完整和准确，但是可以填补批处理高延迟导致的数据的空白。加速层可以使用 Storm/Spark streaming/Flink等计算框架 Serving Layer: 合并服务层 合并层将批处理和加速层的的数据合并，输出出来或者提供给下游来分析  IBM 使用的一套Lambda
Lambda 的出现，很好地解决了离线与实时处理二者都能发挥出功效，离线批处理 和 实时数据 都体现了各自的优势，晚上可以跑离线任务，而实时任务一般也是集中在白天，让实时成本可控，且错开了高峰时间
不过随着时代的发展，Lambda 面对当前复杂的业务分析需求逐渐力不从心，暴露出了以下几个问题：
 实时与批量计算结果不一致引起的数据口径不一致 批量计算在晚上计算窗口内无法完成 开发和维护复杂，烟囱式开发没份数据需要至少处理2次 服务器内存大  Kappa框架 Kappa架构 简化了Lambda架构。Kappa架构系统是删除了批处理系统的架构。要取代批处理，数据只需通过流式传输系统快速提供：
那如何用流计算系统对全量数据进行重新计算，步骤如下：
  用Kafka或类似的分布式队列保存数据，需要几天数据量就保存几天。
  当需要全量计算时，重新起一个流计算实例，从头开始读取数据进行处理，并输出到一个结果存储中。
  当新的实例完成后，停止老的流计算实例，并把老的一引起结果删除。
  和Lambda架构相比，在Kappa架构下，只有在有必要的时候才会对历史数据进行重复计算，并且实时计算和批处理过程使用的是同一份代码。或许有些人会质疑流式处理对于历史数据的高吞吐量会力不从心，但是这可以通过控制新实例的并发数进行改善。
Kappa架构的核心思想包括以下三点：
  用Kafka或者类似的分布式队列系统保存数据，你需要几天的数据量就保存几天。
  当需要全量重新计算时，重新起一个流计算实例，从头开始读取数据进行处理，并输出到一个新的结果存储中。
  当新的实例做完后，停止老的流计算实例，并把老的一些结果删除。
  Iota 框架 在IOT大潮下，智能手机、PC、智能硬件设备的计算能力越来越强，而业务需求要求数据实时响应需求能力也越来越强，过去传统的中心化、非实时化数据处理的思路已经不适应现在的大数据分析需求，我提出新一代的大数据IOTA架构来解决上述问题，整体思路是设定标准数据模型，通过边缘计算技术把所有的计算过程分散在数据产生、计算和查询过程当中，以统一的数据模型贯穿始终，从而提高整体的预算效率，同时满足即时计算的需要，可以使用各种Ad-hoc Query来查询底层数据</description>
    </item>
    
    <item>
      <title>HBase 个人总结</title>
      <link>http://keltoy.github.io/posts/hbase-%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93/</link>
      <pubDate>Wed, 24 Jun 2020 17:39:30 +0000</pubDate>
      
      <guid>http://keltoy.github.io/posts/hbase-%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93/</guid>
      <description>思维导图</description>
    </item>
    
    <item>
      <title>jmap jstack jstat</title>
      <link>http://keltoy.github.io/posts/jmap-jstack-jstat/</link>
      <pubDate>Thu, 21 May 2020 16:03:03 +0000</pubDate>
      
      <guid>http://keltoy.github.io/posts/jmap-jstack-jstat/</guid>
      <description>[toc]
jps (Java Virtual Machine Process) 用来输出JVM 进程状态信息
jps [options] [hostid] 如果不指定 hostid 那么久默认当前服务器
option 命令：
 -q 不输出类名，jar名和传入main方法的参数 -m 输出传入main方法的参数 -l 输出main类或者Jar的权限名 -v 输出传入JVM的参数  jmap（查看内存，对象） 可以输出所有内存中对象的工具，甚至是将VM 中的heap，以二进制输出成文本；打印出某个java进程(pid)内存中的所有对象的情况
jmap [option] pid jmap [option] executable core jmap [option] [service-id@]remote-hostname-or-IP 其中：
 option 选项参数 pid 打印配置信息的进程id executable 产生核心dump的java 可执行文件 core 需要打印配置信息的核心文件 server-id 可选唯一id，如果相同的远程主机上运行了多台调试服务器，可以用来标识服务器 remote-hostname-or-IP 远程调试服务器的主机名或ip  option的取值有：
 &amp;lt;none&amp;gt; 查看进程的内存映像信息 -heap 显示java堆详细信息 -histo[:live] 统计java对象堆的直方图 -clstats 打印类加载器信息 -finalizerinfo 显示在F-Queue队列等待Finalizer线程执行finalizer方法的对象 dump:&amp;lt;dump-options&amp;gt; 生成堆转储快照 F: 当-dump没有响应时， 使用 -dump 或者 -histo参数。在这个模式下,live子参数无效.</description>
    </item>
    
    <item>
      <title>spark shuffle 和数据倾斜</title>
      <link>http://keltoy.github.io/posts/spark-shuffle-%E5%92%8C%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/</link>
      <pubDate>Tue, 12 May 2020 20:39:43 +0000</pubDate>
      
      <guid>http://keltoy.github.io/posts/spark-shuffle-%E5%92%8C%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/</guid>
      <description>[toc]
前言 这是复制过来的第二篇文章，专门讲如何处理数据倾斜问题的 原链接
数据倾斜 现象  绝大多数task 执行非常快，但个别task执行极慢。比如，总共有1000个task，997个task都在1分钟之内执行完了，但是剩余两三个task却要一两个小时。这种情况很常见。 原本能够正常执行的Spark作业，某天突然报出OOM（内存溢出）异常，观察异常栈，是我们写的业务代码造成的。这种情况比较少见。  发生原理 数据倾斜的原理很简单：在进行shuffle的时候，必须将各个节点上相同的key拉取到某个节点上的一个task来进行处理，比如按照key进行聚合或join等操作。此时如果某个key对应的数据量特别大的话，就会发生数据倾斜。
比如大部分key对应10条数据，但是个别key却对应了100万条数据，那么大部分task可能就只会分配到10条数据，然后1秒钟就运行完了；但是个别task可能分配到了100万数据，要运行一两个小时。因此，整个Spark作业的运行进度是由运行时间最长的那个task决定的。
因此出现数据倾斜的时候，Spark作业看起来会运行得非常缓慢，甚至可能因为某个task处理的数据量过大导致内存溢出。
下图就是一个很清晰的例子：hello这个key，在三个节点上对应了总共7条数据，这些数据都会被拉取到同一个task中进行处理；而world和you这两个key分别才对应1条数据，所以另外两个task只要分别处理1条数据即可。此时第一个task的运行时间可能是另外两个task的7倍，而整个stage的运行速度也由运行最慢的那个task所决定。
定位代码 数据倾斜只会发生在shuffle过程中。一些常用的并且可能会触发shuffle操作的算子：
 distinct groupByKey reduceByKey aggregateByKey join cogroup repartition等。  出现数据倾斜时，可能就是你的代码中使用了这些算子中的某一个所导致的。
某个task执行特别慢的情况 首先要看的，就是数据倾斜发生在第几个stage中。
可以通过Spark Web UI来查看当前运行到了第几个stage。此外，无论是使用yarn-client模式还是yarn-cluster模式，我们都可以在Spark Web UI上深入看一下当前这个stage各个task分配的数据量，从而进一步确定是不是task分配的数据不均匀导致了数据倾斜。
知道数据倾斜发生在哪一个stage之后，接着我们就需要根据stage划分原理，推算出来发生倾斜的那个stage对应代码中的哪一部分，这部分代码中肯定会有一个shuffle类算子。精准推算stage与代码的对应关系，需要对Spark的源码有深入的理解，这里我们可以介绍一个相对简单实用的推算方法：只要看到Spark代码中出现了一个shuffle类算子或者是Spark SQL的SQL语句中出现了会导致shuffle的语句（比如group by语句），那么就可以判定，以那个地方为界限划分出了前后两个stage。
这里我们就以Spark最基础的入门程序——单词计数来举例，如何用最简单的方法大致推算出一个stage对应的代码。如下示例，在整个代码中，只有一个reduceByKey是会发生shuffle的算子，因此就可以认为，以这个算子为界限，会划分出前后两个stage。
 stage0，主要是执行从textFile到map操作，以及执行shuffle write操作。shuffle write操作，我们可以简单理解为对pairs RDD中的数据进行分区操作，每个task处理的数据中，相同的key会写入同一个磁盘文件内。 stage1，主要是执行从reduceByKey到collect操作，stage1的各个task一开始运行，就会首先执行shuffle read操作。执行shuffle read操作的task，会从stage0的各个task所在节点拉取属于自己处理的那些key，然后对同一个key进行全局性的聚合或join等操作，在这里就是对key的value值进行累加。stage1在执行完reduceByKey算子之后，就计算出了最终的wordCounts RDD，然后会执行collect算子，将所有数据拉取到Driver上，供我们遍历和打印输出。  val conf = new SparkConf() val sc = new SparkContext(conf) val lines = sc.textFile(&amp;#34;hdfs://...&amp;#34;) val words = lines.flatMap(_.split(&amp;#34; &amp;#34;)) val pairs = words.</description>
    </item>
    
    <item>
      <title>spark调优</title>
      <link>http://keltoy.github.io/posts/spark%E8%B0%83%E4%BC%98/</link>
      <pubDate>Tue, 12 May 2020 15:35:34 +0000</pubDate>
      
      <guid>http://keltoy.github.io/posts/spark%E8%B0%83%E4%BC%98/</guid>
      <description>Spark 调优 [toc]
前言 发现两篇古老的调优数据，复制一下 原链接
避免创建重复的RDD 通常来说，在开发Spark作业时，
 首先是基于某个数据源创建一个初始RDD 对这个RDD执行算子操作，得到下一个RDD 以此类推，循环上述步骤 得出最终结果  这个过程中通过不同的算子操作(map, reduce等)串起多个RDD,就是 RDD lineage，也就是RDD的血缘关系链
需要注意的是： 对于同一份数据，应该只创建一个RDD，不能创建多个RDD来代表同一个数据
同一份数据创建了多个RDD意味着 spark作业会进行多次重复计算来创建多个代表相同数据的RDD，进而增加了作业的性能开销
// 需要对名为“hello.txt”的HDFS文件进行一次map操作，再进行一次reduce操作。也就是说，需要对一份数据执行两次算子操作。  // 错误的做法：对于同一份数据执行多次算子操作时，创建多个RDD。 // 这里执行了两次textFile方法，针对同一个HDFS文件，创建了两个RDD出来，然后分别对每个RDD都执行了一个算子操作。 // 这种情况下，Spark需要从HDFS上两次加载hello.txt文件的内容，并创建两个单独的RDD；第二次加载HDFS文件以及创建RDD的性能开销，很明显是白白浪费掉的。 val rdd1 = sc.textFile(&amp;#34;hdfs://192.168.0.1:9000/hello.txt&amp;#34;) rdd1.map(...) val rdd2 = sc.textFile(&amp;#34;hdfs://192.168.0.1:9000/hello.txt&amp;#34;) rdd2.reduce(...) // 正确的用法：对于一份数据执行多次算子操作时，只使用一个RDD。 // 这种写法很明显比上一种写法要好多了，因为我们对于同一份数据只创建了一个RDD，然后对这一个RDD执行了多次算子操作。 // 但是要注意到这里为止优化还没有结束，由于rdd1被执行了两次算子操作，第二次执行reduce操作的时候，还会再次从源头处重新计算一次rdd1的数据，因此还是会有重复计算的性能开销。 // 要彻底解决这个问题，必须结合“原则三：对多次使用的RDD进行持久化”，才能保证一个RDD被多次使用时只被计算一次。 val rdd1 = sc.textFile(&amp;#34;hdfs://192.168.0.1:9000/hello.txt&amp;#34;) rdd1.map(...) rdd1.reduce(...) 尽可能复用一个RDD 在不同的数据执行算子操作时，还要尽可能复用一个RDD。数据有重叠、或者有包含的情况下，应该减少RDD的数量，尽可能减少算子的执行次数
// 错误的做法。  // 有一个&amp;lt;Long, String&amp;gt;格式的RDD，即rdd1。 // 接着由于业务需要，对rdd1执行了一个map操作，创建了一个rdd2，而rdd2中的数据仅仅是rdd1中的value值而已，也就是说，rdd2是rdd1的子集。 val rdd1: RDD[(Long, String)] = .</description>
    </item>
    
    <item>
      <title>maven plugins</title>
      <link>http://keltoy.github.io/posts/maven-plugins/</link>
      <pubDate>Wed, 15 Apr 2020 22:10:16 +0000</pubDate>
      
      <guid>http://keltoy.github.io/posts/maven-plugins/</guid>
      <description>maven plugins [toc]
manven-enforcer-plugin 功能 在项目validate的过程时，对项目环境进行检查
原理 enforcer 配置之后会默认在validate 后执行enforcer:enforce，对项目环境进行检查
使用 &amp;lt;build&amp;gt; &amp;lt;plugins&amp;gt; &amp;lt;plugin&amp;gt; &amp;lt;artifactId&amp;gt;maven-enforcer-plugin&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;1.4.1&amp;lt;/version&amp;gt; &amp;lt;executions&amp;gt; &amp;lt;execution&amp;gt; &amp;lt;!-- 执行实例的id --&amp;gt; &amp;lt;id&amp;gt;default-cli&amp;lt;/id&amp;gt; &amp;lt;goals&amp;gt; &amp;lt;!-- 执行的命令 --&amp;gt; &amp;lt;goal&amp;gt;enforce&amp;lt;/goal&amp;gt; &amp;lt;/goals&amp;gt; &amp;lt;!-- 执行的阶段 --&amp;gt; &amp;lt;phase&amp;gt;validate&amp;lt;/phase&amp;gt; &amp;lt;configuration&amp;gt; &amp;lt;!-- 制定的规则 --&amp;gt; &amp;lt;rules&amp;gt; &amp;lt;!-- 制定jdk版本 --&amp;gt; &amp;lt;requireJavaVersion&amp;gt; &amp;lt;!-- 执行失败后的消息提示 --&amp;gt; &amp;lt;message&amp;gt; &amp;lt;![CDATA[You are running an older version of Java. This application requires at least JDK ${java.version}.]]&amp;gt; &amp;lt;/message&amp;gt; &amp;lt;!-- jdk版本规则 --&amp;gt; &amp;lt;version&amp;gt;[${java.version}.0,)&amp;lt;/version&amp;gt; &amp;lt;/requireJavaVersion&amp;gt; &amp;lt;/rules&amp;gt; &amp;lt;/configuration&amp;gt; &amp;lt;/execution&amp;gt; &amp;lt;/executions&amp;gt; &amp;lt;/plugin&amp;gt; &amp;lt;/plugins&amp;gt; &amp;lt;/build&amp;gt; &amp;lt;properties&amp;gt; &amp;lt;java.</description>
    </item>
    
    <item>
      <title>conda 命令</title>
      <link>http://keltoy.github.io/posts/conda-%E5%91%BD%E4%BB%A4/</link>
      <pubDate>Wed, 08 Apr 2020 13:43:00 +0000</pubDate>
      
      <guid>http://keltoy.github.io/posts/conda-%E5%91%BD%E4%BB%A4/</guid>
      <description>前言 之前都是使用 pip 和 virtualenv 管理python 的包管理和环境管理，最近体验了一下 jupyterlab， 好多插件的使用都需要安装nodejs，virtualenv 只能管理python 的环境，于是就想到了conda 试试conda 的环境管理能不能，惊喜的发现可以，所以以后的使用应该都迁移到 conda上来了
conda简介 说明原文conda 文档
conda 是开源的跨操作系统的包管理系统和环境管理系统，可以管理多种语言，运行安装都比较简单
conda 包管理 # 添加一个渠道，获取软件包的渠道 常用的有 bioconda, conda-forge, genomedk conda config --add channel # 设置去掉url显示 conda config --set show_channel_urls yes # 渠道列表 conda config --get channels # 搜索包 conda search [-c channel] packagename # 安装包 conda install packagename=versionnumber # 包列表 conda list # 删除包 conda remove packagename conda 环境管理 # 环境列表 conda env --list # 环境信息 conda info --envs # 创建python3环境 conda create -n name python=3 # 激活环境 conda activate environmentname # 退出环境 conda deactivate # 删除环境 conda remove -n environmentname --all </description>
    </item>
    
    <item>
      <title>Shell 的笔记</title>
      <link>http://keltoy.github.io/posts/shell-%E7%9A%84%E7%AC%94%E8%AE%B0/</link>
      <pubDate>Thu, 04 Jul 2019 11:13:56 +0000</pubDate>
      
      <guid>http://keltoy.github.io/posts/shell-%E7%9A%84%E7%AC%94%E8%AE%B0/</guid>
      <description>Preface 翻开上学的时候破破烂烂的本子，看到刚开始学习 linux 的一些shell 总结，还是将这些记到网上好一些，有些还是挺实用的，我感觉
慢慢更新 &amp;hellip; $[] $(()) expr 这两个括号可以执行基本的算数操作（也就是 + - * / ）
#!/bin/sh bash result=3 r=4 echo $(($r + $result)) echo $[$r + $result] echo `expr 3 + 4`  注意的是空格，赋值语句 = 两边不能有 空格；expr 后的表达式需要 空格
 bc 具体功能就是调用计算器
&amp;#34; 4 * 0.56 &amp;#34;|bc  依然注意 空格 bc 前的|不能跟空格
 标准输入输出    符号 说明     0 stadin 标准输入   1 stadout 标准输出   2 staderr 错误输出   &amp;gt; 覆盖写入重定向   &amp;raquo; 追加写入   &amp;lt; 读取文件    cmd 2&amp;gt;stderr.</description>
    </item>
    
    <item>
      <title>virtualenv 小记</title>
      <link>http://keltoy.github.io/posts/virtualenv-%E5%B0%8F%E8%AE%B0/</link>
      <pubDate>Mon, 01 Jul 2019 17:15:27 +0000</pubDate>
      
      <guid>http://keltoy.github.io/posts/virtualenv-%E5%B0%8F%E8%AE%B0/</guid>
      <description>前言 最近想要获取url上的固定每日数据，写到原始脚本里发现程序卡死了，思前想后想再写一个脚本，这样也不用怎么修改之前的脚本。因为在服务器上运行还需要一些权限，所以希望能够使用virtualenv配置一下自己的环境。
开始实践 在服务器上安装 vitualenv, 然后创建环境就可以运行了
pip install virtualenv virtualenv --no-site-packages venv  &amp;ndash;no-site-packages 代表安装虚拟环境的时候不需要任何其他多余的包
 自己对虚拟环境的误解 因为是服务器环境，我发现我并没有root权限不能安装任何python的包。 我就希望将自己笔记本的python 环境打好包，安装到服务器上。 于是我将需要的包和库安装好后，使用virtualenv 配置好，打包传送到服务器上，准备使用virtualenv 开始运行
./venv/bin/activate 服务端的前缀出现了python3 的小括号，感觉目前都比较顺利。 然后查看版本号
python -V 2.7.5 怎么回事？没有变化？应该是变化成了virtualenv 中的 3.7才对 然后调用env中的python 确实是 3.7 于是 更改python 重新运行，发现这个3.7的python 什么包也照得不到。
看来自己对虚拟环境是有所误解
于是检查了一下venv/bin下的 activate 文件，才发现原来这个文件只是做了个映射
VIRTUAL_ENV=&amp;#34;xxx/venv&amp;#34; export VIRTUAL_ENV 所以想让activate 生效， 还需要更改这里的地址。</description>
    </item>
    
    <item>
      <title>有关Spring注释的一些总结</title>
      <link>http://keltoy.github.io/posts/%E6%9C%89%E5%85%B3spring%E6%B3%A8%E9%87%8A%E7%9A%84%E4%B8%80%E4%BA%9B%E6%80%BB%E7%BB%93/</link>
      <pubDate>Tue, 11 Dec 2018 10:50:58 +0000</pubDate>
      
      <guid>http://keltoy.github.io/posts/%E6%9C%89%E5%85%B3spring%E6%B3%A8%E9%87%8A%E7%9A%84%E4%B8%80%E4%BA%9B%E6%80%BB%E7%BB%93/</guid>
      <description>Spring Boot @SpringBootApplication  org.springframework.boot.autoconfigure.SpringBootApplication 项目的引导类 使用 SpringApplication.run(类名.class, args)进行启动，org.springframework.boot.SpringApplication 会返回 ApplicationContext对象  @Bean  使用@Component @Service 或者 @repository 标注一个Java类可以定义一个Bean 使用@Configuration 注解标签来标注一个类，然后在每个要构建的Bean定义一个构造器，在构造器方法上添加@Bean来定义一个Bean  Actuator 在maven中引入
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;spring-boot-starter-actuator&amp;lt;/artifactId&amp;gt; &amp;lt;/dependency&amp;gt; 然后可以配置
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;io.micrometer&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;micrometer-registry-prometheus&amp;lt;/artifactId&amp;gt; &amp;lt;/dependency&amp;gt; 默认在 /actuator 里之开启了health和info，需要在 application的配置里面添加
management: endpoints: web: exposure: include: health, info, env, metrics endpoint: health: show-details: always </description>
    </item>
    
    <item>
      <title>Synchronized</title>
      <link>http://keltoy.github.io/posts/synchronized/</link>
      <pubDate>Thu, 18 Oct 2018 16:46:11 +0000</pubDate>
      
      <guid>http://keltoy.github.io/posts/synchronized/</guid>
      <description>前言 Java 中 synchronized 关键字 在 很早以前就已经有了，我还记得自己刚开始学习 Java 的时候，别人问我并发的时候我都是说，加个 synchronized 就好了，温故而知新，现在重新学习 synchronized 关键字。
正文 注意的是，synchronized 是可重入锁！！
synchronized 同步基础 Java 中的对象创建时就有了监视器，每个 Java 对象都可以作为锁，
 普通同步方法，锁就是当前实力对象(this对象) 静态同步方法，锁是当前Class对象(.class对象) 同步方法块，锁是 synchronized 括号内部配置的对象  所以说，synchronized 锁住的都是对象。
synchronized 实现原理 Synchronized 代码块的实现是通过监视器实现的。使用 monitorenter 和 monitorexit 这两个指令来实现的，而且这两条指令必须成对出现。
  monitorenter 指令在编译后插入到同步代码块的开始位置
  monitorexit 指令是出入在异常处和结束位置
  Java 对象头 Synchronized 锁是存在 Java对象头里的。
 如果是对象，占用2个字来存储 如果是数组，占用3个字来存储     长度 内容 说明     32/64bit Mark Word 存储对象的hashCode或者锁信息   32/64bit Class Metadata Address 存储到对象类型的指针   32/64bit Array length 数组才有的选项，存储数组的长度    锁的升级 Synchronized 锁一共有4种状态，逐次升级。 级别由低到高：</description>
    </item>
    
    <item>
      <title>ConcurrentHashMap</title>
      <link>http://keltoy.github.io/posts/concurrenthashmap/</link>
      <pubDate>Tue, 16 Oct 2018 09:31:33 +0000</pubDate>
      
      <guid>http://keltoy.github.io/posts/concurrenthashmap/</guid>
      <description>前言 重新学习ConcurrentHashMap
正题 使用ConcurrentHashMap的原因  ConcurrentHashMap 是 线程安全且高效的 HashMap 并发编程中使用HashMap可能导致程序死循环 HashTable 效率低下  HashMap 导致死循环 final HashMap&amp;lt;String, String&amp;gt; map = new Hashmap&amp;lt;String, String&amp;gt;(2); Thread t new Thread(new Runnable() { @Override public void run() { for (int i = 0; i &amp;lt; 10000; i++) { new Thread(new Runnable() { @Override public void run() { map.put(UUID.randomUUID().toString(), &amp;#34;&amp;#34;); } }, &amp;#34;ftf&amp;#34;+i).start(); } } }, &amp;#34;ftf&amp;#34;); t.start(); t.join(); 在多线程的情况下，HashMap 的 Entry 链表可能会形成有环形的数据结构，那么 Entry 的 next 节点 就不可能为空，那就有可能无限循环获取 Entry。</description>
    </item>
    
    <item>
      <title>机器学习基本点</title>
      <link>http://keltoy.github.io/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E7%82%B9/</link>
      <pubDate>Fri, 01 Jun 2018 10:09:37 +0000</pubDate>
      
      <guid>http://keltoy.github.io/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E7%82%B9/</guid>
      <description>前言 这是第二遍开始学习机器学习，第一遍都记到本子上，这一次在这里记一次，看看两次记录有哪些异同
三要素 模型  模型可以是条件概率分布 模型可以是决策函数  策略  策略就是考虑按照什么样的规则学习或者选择最优模型。 度量模型的好坏可以使用损失函数和风险函数  损失函数 损失函数度量一次预测的好坏
风险函数 风险函数度量平均意义下模型预测的好坏
损失函数与代价函数的区别  损失函数：用来计算单个样本的误差 代价函数：与损失函数一样也是计算样本误差的，网上说区别是，代价函数是计算训练集上的所有样本的误差平均数（不过这就跟风险函数重复了不是么？） 目标函数：一般来说就是 损失函数 + 正则化项  损失函数 损失函数也分了好几种，通常使用 $L(Y, f(X))$
 0-1损失函数  $$ L(Y, f(X)) =\begin{cases} 1&amp;amp; Y \neq f(X)\
0&amp;amp; Y = f(X) \end{cases} $$
平方损失函数 主要用于最小二乘法  $$ L(Y, f(X)) = {(Y-f(X))}^2 $$
绝对损失函数  $$ L(Y, f(X)) = \vert Y-f(X) \vert $$
对数(似然)损失函数  $$ L(Y, P(Y|X)) = -\log P(Y|X) $$</description>
    </item>
    
    <item>
      <title>Redis 并发小结</title>
      <link>http://keltoy.github.io/posts/redis-%E5%B9%B6%E5%8F%91%E5%B0%8F%E7%BB%93/</link>
      <pubDate>Sat, 29 Apr 2017 12:12:58 +0000</pubDate>
      
      <guid>http://keltoy.github.io/posts/redis-%E5%B9%B6%E5%8F%91%E5%B0%8F%E7%BB%93/</guid>
      <description>Preface 实习了一段时间，一直没有写东西了，最近清闲了一些，开始总结一些实习上所获取的经验。不是很高深，但是可能经常会用到。
应用场景 比如要在网上买理财产品，每个理财产品都是有限定金额的。这个产品会有一个总金额 totalAmount，每个人都可以买，也不限制买多少，但是所有人的购买金额不能超过这个总金额。这是一个很简单的场景，之前没有接触缓存的时候处理也比较简单。这些用户购买的数据是存到数据库中的，因此可以通过数据库直接控制。
使用数据库锁进行处理 使用数据库的共享锁和排他锁是可以实现数据的同步的。但是我没有实际实验过，所有人都说尽量不要这么去实现，效率会低很多。
共享锁： SELECT ... LOCK IN SHARE MODE; 排他锁： SELECT ... FOR UPDATE; 个人的理解，共享锁(S)和排他锁(X)就是读写锁，这种形式在读多写少的情况下会比较高效。
使用数据库的乐观并发机制 使用乐观并发机制，可以在相应的表中添加一个 version 列，每次在购买前获取一下 version，然后进行购买时检查version是否有变化，如果有变化就不进行处理，如果没有变化，在确保成功后更改 version。
当然，在我所举例的例子中，这个 version 可以直接是 总金额 totalAmount，每次操作的时候都对这个总金额进行改变，如果总金额小于0了，说明不能进行处理，回滚处理，购买不成功
看似是个不错的方法，但是有很多的限制。如果数据库不允许修改，而一开始并没有总金额或者 version 这样的列呢，用 updatedtime 也能够实现，但是据说用时间索引效率会低很多。如果数据库分库分表了，又该如何控制？
使用Redis进行并发控制 setnx setnx 是一个事务操作，会先检查缓存中是否存在，如果不存在则设置，否则不做更改。 这样设计不会出现没有volatile 双重检测出现的那样的问题，不会在判断有无之后，插入数据之前的时间插入另外的数据。那么使用 setnx 的用法就与 mutex非常类似，在插入前 setnx，确认成功之后，进行 del。
setnx key value ... del key 在此之前，我也想过使用 incrby 对数据进行增加和减少，但是，在这种情况下，防不住并发。
一点思考 假设现在需要做一点改变。假设每个人最多只能购买三次，第四次购买就失败。如果这样，如果不允许在数据库中需要添加一个 purchase 的列，怎么使用redis 进行设计？ 同样是使用 setnx，然后在内部再使用一个 incrby 是否就安全了？
消息队列 如果使用消息队列，能够保证数据是一个个到来的，那么是不是也可以保证并发，而且降低了复杂度？但是对于一个简单的需求，使用消息的代价也不小。
Postscript 将传统项目放到分布式架构上，就容易出现问题。分布式的消息传递代价也不小，因此，并发这块一直也很热。</description>
    </item>
    
    <item>
      <title>Emacs: org-mode</title>
      <link>http://keltoy.github.io/posts/emacs-org-mode/</link>
      <pubDate>Wed, 18 Jan 2017 11:20:41 +0000</pubDate>
      
      <guid>http://keltoy.github.io/posts/emacs-org-mode/</guid>
      <description>Time and tide wait for no man
 Preface 已经听了很多人说 org-mode 非常好用，一直不知道怎么用，今天看了看一些文章，发现其实和 markdown 有那么点像，总结总结，想自己做个 GTD。
Org-mode Chapter md 中的章节使用 &amp;ldquo;#&amp;quot;，而 org-mode 中使用 &amp;ldquo;*&amp;quot;。
不过，org-mode 有一些有趣的操作：
 S-tab, toggle 所有的 chapter tab, toggle 当前的 chapter M-left/right, 升级/降级 chapter M-up/down, 调整 chapter 的顺序  List 无序 md 无序队列使用 &amp;ldquo;*&amp;rdquo; 和 &amp;ldquo;+&amp;quot;，而 org-mode 使用 &amp;ldquo;+&amp;rdquo; 和 &amp;ldquo;-&amp;quot;。
注意到的是 md 的间距是不同的：
 这是 &amp;ldquo;*&amp;rdquo; 的第一行 这是第二行   这是 &amp;ldquo;+&amp;rdquo; 的第一行 这是第二行  有序 md 的有序使用的是 &amp;ldquo;1.</description>
    </item>
    
    <item>
      <title>Question For Nginx Error 500</title>
      <link>http://keltoy.github.io/posts/question-for-nginx-error-500/</link>
      <pubDate>Tue, 10 Jan 2017 12:23:20 +0000</pubDate>
      
      <guid>http://keltoy.github.io/posts/question-for-nginx-error-500/</guid>
      <description>Preface 前几天服务器到期了，然后重新申请了之后，发现本地的 node 也出了问题，按照网上的方法也修理不好。
于是自己想使用 docker 来重新建。费了一些时间把 docker 实践了，感觉挺费流量的&amp;hellip;不过还好。
昨天，node 自己又好了&amp;hellip;优点莫名其妙，后来想想可能跟 python2.7.x 的版本有关系。
然后重新搭建的时候发现 nginx 启动不了， 返回
500 internal server error  500 internal server error 使用 systemctl status nginx.service 发现，启动之后访问 web 目录访问不了。
网上说跟自己的配置有关系，比如 location 配置的有问题等等。
本人检查了很久没有什么问题，确定分号，拼写都没问题，但是还是会报 500 错误。
Log is key 一筹莫展的时候就想试试各种办法，于是就查询了 /var/log/nginx 下面的错误日志，发现报的错误是
13: Permission denied  难道是权限不够？对于 755 的权限设置应该是没有问题，于是我改成了 777 虽然觉得没什么效果，但是还是试了一试。 果然，没有效果，依旧拒绝。不过可以确定的是，应该是权限的问题。
可能，是用户的问题。带着这个想法我看了一下 nginx.conf 的用户发现
user nginx  试试给个最大的权限 修改成 root
user root  重启一下 nginx
OK！运行成功了。</description>
    </item>
    
    <item>
      <title>Index For MySQL</title>
      <link>http://keltoy.github.io/posts/index-for-mysql/</link>
      <pubDate>Fri, 30 Dec 2016 23:16:42 +0000</pubDate>
      
      <guid>http://keltoy.github.io/posts/index-for-mysql/</guid>
      <description>Preface  Beautiful is better than ugly.
 之前被问到索引的分类，被分为唯一性索引和普通索引，有点懵逼。回头再看看，总结一下。
Keys  PRIMARY 主键索引建立主键索引其实跟 UNIQUE 没什么区别 INDEX 普通的索引 UNIQUE 唯一性索引 FULLTEXT 全文索引，这个 innodb 在 MySQL 5.5 之前不支持 SPAIAL 空间索引  这么分其实很混乱，这几个 key 好像不是一个维度的。
Types of Index 在《高性能 MySQL》是这么分的：
 B-Tree 索引 哈希索引 空间数据索引 全文索引 其他索引  </description>
    </item>
    
    <item>
      <title>Learning Docker</title>
      <link>http://keltoy.github.io/posts/learning-docker/</link>
      <pubDate>Thu, 15 Dec 2016 23:59:23 +0000</pubDate>
      
      <guid>http://keltoy.github.io/posts/learning-docker/</guid>
      <description>What Is Docker Docker 是开源的应用容器引擎，开发者可以打包他们的应用以及依赖包到一个可移植的容器中，然后发不到其他机器上，实现虚拟化。容器完全使用沙箱机制，互相之间不会有任何接口。 Docker 基于LXC的引擎使用 go 开发。
How To Use Docker  构建一个镜像 运行容器  其实可以当作一个虚拟机来使用&amp;hellip;.
Docker Command docker info 查看 docker 的相关信息
docker pull an image or a repository 从远端拉取一个镜像或者仓库 eg:
docker pull busybox  docker run image cmd 运行镜像的一个命令 eg:
docker run busybox /bin/echo Hello Docker  eg: 后台进程方式运行
sample_job=$(docker run -d busybox /bin/sh -c &amp;quot;while true; do echo Docker; sleep 1; done&amp;quot;)  -d 代表 detach</description>
    </item>
    
    <item>
      <title>Source Code in Java -- Spring IOC - II</title>
      <link>http://keltoy.github.io/posts/source-code-in-java-spring-ioc-ii/</link>
      <pubDate>Thu, 10 Nov 2016 23:09:39 +0000</pubDate>
      
      <guid>http://keltoy.github.io/posts/source-code-in-java-spring-ioc-ii/</guid>
      <description>Preface  简单来说 IoC 容器的初始化是由 refresh() 方法启动的，这个方法标志着 IoC 容器正式启动。具体来说这个启动包括 BeanDefinition 的 Resource 定位、载入和注册三个基本过程。
 关于这段话我首先不能理解的就是无缘无故出来的这个BeanDefinition，我在浏览这些源码的时候也没看到。因此，为了好好了解 Spring IoC，我还需要进一步查看内部源码和解释。
What is BeanDefinition?  对 IoC 来说，BeanDefinition 就是对依赖反转模式中管理的对象依赖关系的数据抽象。
Spring 通过定义 BeanDefinition 来管理基于 Spring 的应用中的各种对象以及它们之间的相互依赖关系。BeanDefinition 抽象了我们对 Bean 的定义，是让容器起作用的主要数据类型。
 我的理解 BeanDefinition 有点像 Bean 的元数据，又有点像抽象类，又有点像 schema。
Resource location of BeanDefinition 再次拿出来这幅图来讲一下，像 ClassPathXmlApplicationContext 这样的方法定位资源的方式还是使用的 DefaultResourceLoader。而这个 DefaultResourceLoader 实现的是 ResourceLoader 接口：
public interface ResourceLoader { /** Pseudo URL prefix for loading from the class path: &amp;#34;classpath:&amp;#34; */ String CLASSPATH_URL_PREFIX = ResourceUtils.</description>
    </item>
    
    <item>
      <title>Emacs Tutorials</title>
      <link>http://keltoy.github.io/posts/emacs-tutorials/</link>
      <pubDate>Sun, 06 Nov 2016 23:20:48 +0000</pubDate>
      
      <guid>http://keltoy.github.io/posts/emacs-tutorials/</guid>
      <description>Preface 笔记本上的东西还是要放到网上的，当作一个备份了。一开始学习 emacs 的时候有很多坑要踩，现在好了，基本适应了，以后多用用应该问题不大了。
Emacs    标记 代表按键     C- Control   S- Shift   M- Alt(Option) ／ ESC   RET Return   SPC Space   DEL Backspace(Delete)    总的来说，无论是装逼需要，还是提高效率，Emacs 的学习我个人认为还是挺有用的，慢慢习惯了这个方式之后，其实会发现，想问题的思路被打开了。
几个简单的指令，也是最常用的
   指令 命令名称 说明     M-x execute-extended-command 执行命令   C-u (#)/M-(#) key  重复#次 key   C-g keyboard-quit 停止当前输入   C-x u undo 撤销命令   S-u revert-buffer 撤销上次保存后的所有改动    recover-file 从自动保存文件中恢复    recover-session 恢复此次会话所有文件   &amp;lt;f10&amp;gt; menu-bar-open 打开菜单栏    Help    指令 命令名称 说明     C-h ?</description>
    </item>
    
    <item>
      <title>Source Code in Java -- Spring IoC</title>
      <link>http://keltoy.github.io/posts/source-code-in-java-spring-ioc/</link>
      <pubDate>Wed, 02 Nov 2016 23:46:46 +0000</pubDate>
      
      <guid>http://keltoy.github.io/posts/source-code-in-java-spring-ioc/</guid>
      <description>Preface Spring 的源码在网上基本上都被研究透了，我之前的理解实际上也是基于书本和基于一些项目。既然是阅读源码，那么 Spring 的源码是不得不看的。
Spring IoC 设计 网上找的一张图，和书上的一样，先拿过来用了：
挺多的，但是不复杂，从名字就可以看出来每个类或者接口。
 这个接口系统是以 BeanFactory 和 ApplicationContext 为核心的。
 因此主要也是介绍这两个接口。
BeanFactory  提供的是最基本的 IoC 容器功能 可以使用转义符“&amp;amp;”获取FactoryBean 设计的 getBean 方法是使用 IoC 容器的主要方法。  对于这个转义符，我没弄明白，书上是这么介绍的：
 用户使用容器时，可以使用转义符“&amp;amp;”来得到 FactoryBean 本身，用来区分通过容器来获取 FactoryBean 产生的对象和获取 FacoryBean 产生的对象和获取 FactoryBean 本身。
 简单来说，就是加上“&amp;amp;”之后获取的是 FactoryBean，而不是 FactoryBean 产生的对象。
需要注意的是 BeanFactory 是 IoC容器或者对象工厂，FactoryBean 是 Bean。
public interface BeanFactory { /** * Used to dereference a {@link FactoryBean} instance and distinguish it from * beans &amp;lt;i&amp;gt;created&amp;lt;/i&amp;gt; by the FactoryBean.</description>
    </item>
    
    <item>
      <title>Source Code in Java -- ArrayList</title>
      <link>http://keltoy.github.io/posts/source-code-in-java-arraylist/</link>
      <pubDate>Tue, 25 Oct 2016 23:46:28 +0000</pubDate>
      
      <guid>http://keltoy.github.io/posts/source-code-in-java-arraylist/</guid>
      <description>前言 ArrayList，相信大家都不陌生了，它的源码网上也是到处都是，但是我还是想看看，也许会有不一样的发现。
初识 ArrayList ArrayList，顺序列表，非线程安全的，基本用法和数组类似，但是可以扩容。默认初始大小为 10， 扩容大小为 1.5 倍。基本上就这么多。本身构造也简单。
思考 ArrayList 为什么会出现 ArrayList 这个类呢，数组不能够满足要求吗？除了可以扩容，他跟数组有什么区别呢？ 扩容为什么是 1.5 倍？ 初始大小为什么是 10？
ArrayList 源码 ArrayList 变量 基于 1.8 的源码，基本上也没太多可说的，主要针对查找、添加、删除、以及扩容方面进行了解。 首先是 ArrayList 的属性。
public class ArrayList&amp;lt;E&amp;gt; extends AbstractList&amp;lt;E&amp;gt; implements List&amp;lt;E&amp;gt;, RandomAccess, Cloneable, java.io.Serializable { private static final long serialVersionUID = 8683452581122892189L; /** * Default initial capacity. */ private static final int DEFAULT_CAPACITY = 10; /** * Shared empty array instance used for empty instances. */ private static final Object[] EMPTY_ELEMENTDATA = {}; /** * Shared empty array instance used for default sized empty instances.</description>
    </item>
    
    <item>
      <title>Spring Configuration-applicationContext-service.xml &amp; SqlMapConfig.xml</title>
      <link>http://keltoy.github.io/posts/spring-configuration-applicationcontext-service-xml-sqlmapconfig-xml/</link>
      <pubDate>Wed, 19 Oct 2016 22:29:47 +0000</pubDate>
      
      <guid>http://keltoy.github.io/posts/spring-configuration-applicationcontext-service-xml-sqlmapconfig-xml/</guid>
      <description>前言 配置的问题就到尾声了。使用和了解还是不一样，内部原理实现更是复杂，因此我觉得主要问题还是要放到源码的阅读上。
applicationContext.xml 基本上该配置的都配置了，service 是提供服务的，也不用配别的了，就是配置一下包扫描器，扫描一下 service 包：
&amp;lt;?xml version=&amp;#34;1.0&amp;#34; encoding=&amp;#34;UTF-8&amp;#34;?&amp;gt; &amp;lt;beans xmlns=&amp;#34;http://www.springframework.org/schema/beans&amp;#34; xmlns:xsi=&amp;#34;http://www.w3.org/2001/XMLSchema-instance&amp;#34; xmlns:context=&amp;#34;http://www.springframework.org/schema/context&amp;#34; xsi:schemaLocation=&amp;#34;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd&amp;#34;&amp;gt; &amp;lt;context:component-scan base-package=&amp;#34;com.summer.service&amp;#34;/&amp;gt; &amp;lt;/beans&amp;gt; 还有redis 的配置：
&amp;lt;bean class=&amp;#34;redis.clients.jedis.JedisCluster&amp;#34; id=&amp;#34;jedisCluster&amp;#34;&amp;gt; &amp;lt;constructor-arg&amp;gt; &amp;lt;set&amp;gt; &amp;lt;bean class=&amp;#34;redis.clients.jedis.HostAndPort&amp;#34;&amp;gt; &amp;lt;constructor-arg name=&amp;#34;host&amp;#34; value=&amp;#34;172.21.14.118&amp;#34; /&amp;gt; &amp;lt;constructor-arg name=&amp;#34;port&amp;#34; value=&amp;#34;7001&amp;#34;/&amp;gt; &amp;lt;/bean&amp;gt; &amp;lt;bean class=&amp;#34;redis.clients.jedis.HostAndPort&amp;#34;&amp;gt; &amp;lt;constructor-arg name=&amp;#34;host&amp;#34; value=&amp;#34;172.21.14.118&amp;#34; /&amp;gt; &amp;lt;constructor-arg name=&amp;#34;port&amp;#34; value=&amp;#34;7002&amp;#34;/&amp;gt; &amp;lt;/bean&amp;gt; &amp;lt;bean class=&amp;#34;redis.clients.jedis.HostAndPort&amp;#34;&amp;gt; &amp;lt;constructor-arg name=&amp;#34;host&amp;#34; value=&amp;#34;172.21.14.118&amp;#34; /&amp;gt; &amp;lt;constructor-arg name=&amp;#34;port&amp;#34; value=&amp;#34;7003&amp;#34;/&amp;gt; &amp;lt;/bean&amp;gt; &amp;lt;bean class=&amp;#34;redis.clients.jedis.HostAndPort&amp;#34;&amp;gt; &amp;lt;constructor-arg name=&amp;#34;host&amp;#34; value=&amp;#34;172.21.14.118&amp;#34; /&amp;gt; &amp;lt;constructor-arg name=&amp;#34;port&amp;#34; value=&amp;#34;7004&amp;#34;/&amp;gt; &amp;lt;/bean&amp;gt; &amp;lt;bean class=&amp;#34;redis.clients.jedis.HostAndPort&amp;#34;&amp;gt; &amp;lt;constructor-arg name=&amp;#34;host&amp;#34; value=&amp;#34;172.</description>
    </item>
    
    <item>
      <title>Spring Configuration-applicationContext-trans.xml</title>
      <link>http://keltoy.github.io/posts/spring-configuration-applicationcontext-trans-xml/</link>
      <pubDate>Wed, 19 Oct 2016 22:12:27 +0000</pubDate>
      
      <guid>http://keltoy.github.io/posts/spring-configuration-applicationcontext-trans-xml/</guid>
      <description>前言 数据库的操作，一般都少不了事务，单独一个文件配置事务，是因为确实比较重要。
applicationContext.xml 这里配置的时候需要用到aop的切面操作还有通知，注意要加上正确的地址，否则总会报错。
&amp;lt;?xml version=&amp;#34;1.0&amp;#34; encoding=&amp;#34;UTF-8&amp;#34;?&amp;gt; &amp;lt;beans xmlns=&amp;#34;http://www.springframework.org/schema/beans&amp;#34; xmlns:xsi=&amp;#34;http://www.w3.org/2001/XMLSchema-instance&amp;#34; xmlns:tx=&amp;#34;http://www.springframework.org/schema/tx&amp;#34; xmlns:aop=&amp;#34;http://www.springframework.org/schema/aop&amp;#34; xsi:schemaLocation=&amp;#34;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd&amp;#34;&amp;gt; 基本上就是注意 xmlns:tx 和 xmlns:aop 的位置。 开始配置事务。 先配置事务管理，需要把之前在 dao 层配置的数据库连接池引用过来。
&amp;lt;bean class=&amp;#34;org.springframework.jdbc.datasource.DataSourceTransactionManager&amp;#34; id=&amp;#34;dataSourceTransactionManager&amp;#34;&amp;gt; &amp;lt;property name=&amp;#34;dataSource&amp;#34; ref=&amp;#34;dataSource&amp;#34;/&amp;gt; &amp;lt;/bean&amp;gt; 然后配置通知，配置哪些方法需要通知，这些方法使用什么传播形式。传播形式和会话紧密相关。
&amp;lt;tx:advice transaction-manager=&amp;#34;dataSourceTransactionManager&amp;#34; id=&amp;#34;txAdvice&amp;#34;&amp;gt; &amp;lt;tx:attributes&amp;gt; &amp;lt;tx:method name=&amp;#34;save*&amp;#34; propagation=&amp;#34;REQUIRED&amp;#34;/&amp;gt; &amp;lt;tx:method name=&amp;#34;insert*&amp;#34; propagation=&amp;#34;REQUIRED&amp;#34;/&amp;gt; &amp;lt;tx:method name=&amp;#34;add*&amp;#34; propagation=&amp;#34;REQUIRED&amp;#34;/&amp;gt; &amp;lt;tx:method name=&amp;#34;create*&amp;#34; propagation=&amp;#34;REQUIRED&amp;#34;/&amp;gt; &amp;lt;tx:method name=&amp;#34;delete*&amp;#34; propagation=&amp;#34;REQUIRED&amp;#34;/&amp;gt; &amp;lt;tx:method name=&amp;#34;update*&amp;#34; propagation=&amp;#34;REQUIRED&amp;#34;/&amp;gt; &amp;lt;tx:method name=&amp;#34;find*&amp;#34; propagation=&amp;#34;SUPPORTS&amp;#34;/&amp;gt; &amp;lt;tx:method name=&amp;#34;select*&amp;#34; propagation=&amp;#34;SUPPORTS&amp;#34;/&amp;gt; &amp;lt;tx:method name=&amp;#34;get*&amp;#34; propagation=&amp;#34;SUPPORTS&amp;#34;/&amp;gt; &amp;lt;/tx:attributes&amp;gt; &amp;lt;/tx:advice&amp;gt; 然后配置 aop，使用什执行哪些包的什么返回值的 什么参数需要使用以上的通知。</description>
    </item>
    
    <item>
      <title>Spring Configuration-applicationContext-dao.xml</title>
      <link>http://keltoy.github.io/posts/spring-configuration-applicationcontext-dao-xml/</link>
      <pubDate>Wed, 19 Oct 2016 21:23:55 +0000</pubDate>
      
      <guid>http://keltoy.github.io/posts/spring-configuration-applicationcontext-dao-xml/</guid>
      <description>前言 原本 applicationContext.xml 是一个文件，这里将它拆开是为了更好了解业务的配置。
applicationContext.xml 这里才是真正开始配置 Spring 的参数。 首先还是 xml 文件的头。
&amp;lt;?xml version=&amp;#34;1.0&amp;#34; encoding=&amp;#34;UTF-8&amp;#34;?&amp;gt; &amp;lt;beans xmlns=&amp;#34;http://www.springframework.org/schema/beans&amp;#34; xmlns:xsi=&amp;#34;http://www.w3.org/2001/XMLSchema-instance&amp;#34; xmlns:context=&amp;#34;http://www.springframework.org/schema/context&amp;#34; xsi:schemaLocation=&amp;#34;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd &amp;#34; default-lazy-init=&amp;#34;false&amp;#34;&amp;gt; 依然注意 default-lazy-init=&amp;quot;false&amp;quot; 这一句，有人说不加也可以，是默认的，但是我使用的 Spring4， IDEA 搭建的时候不行。 接下来要配置外在配置，属性持有。这里有一个问题，那就是，在 Spring MVC 我已经配过了，为什么这里还要配一次？这个实际上就是父子容器的问题。
&amp;lt;context:property-placeholder location=&amp;#34;classpath:properties/*.properties&amp;#34;/&amp;gt; 既然是 dao 层配置，那么还需要配置数据库连接池。前两天翻书的时候，发现 druid 还支持 storm。
&amp;lt;bean class=&amp;#34;com.alibaba.druid.pool.DruidDataSource&amp;#34; id=&amp;#34;dataSource&amp;#34; destroy-method=&amp;#34;close&amp;#34;&amp;gt; &amp;lt;property name=&amp;#34;driverClassName&amp;#34; value=&amp;#34;${jdbc.driver}&amp;#34; /&amp;gt; &amp;lt;property name=&amp;#34;username&amp;#34; value=&amp;#34;${jdbc.username}&amp;#34; /&amp;gt; &amp;lt;property name=&amp;#34;password&amp;#34; value=&amp;#34;${jdbc.password}&amp;#34; /&amp;gt; &amp;lt;property name=&amp;#34;url&amp;#34; value=&amp;#34;${jdbc.url}&amp;#34; /&amp;gt; &amp;lt;property name=&amp;#34;maxActive&amp;#34; value=&amp;#34;10&amp;#34; /&amp;gt; &amp;lt;property name=&amp;#34;minIdle&amp;#34; value=&amp;#34;5&amp;#34; /&amp;gt; &amp;lt;/bean&amp;gt; 配置完数据库连接池，还需要配置 SqlSessionFactory, 用来与数据库创建会话。这让我想起来 Hibernate 的一级缓存 session 和二级缓存 sessionFactory。总的来说，就是把连接池和数据库的配置注入进来。</description>
    </item>
    
    <item>
      <title>Spring Configuration-springmvc.xml</title>
      <link>http://keltoy.github.io/posts/spring-configuration-springmvc-xml/</link>
      <pubDate>Wed, 19 Oct 2016 14:25:28 +0000</pubDate>
      
      <guid>http://keltoy.github.io/posts/spring-configuration-springmvc-xml/</guid>
      <description>前言 web.xml 配置好了之后（实际上也可以最后配置），还需要配置 web.xml 里面的涉及到一些文件。 先配置 servlet 标签中涉及到的 springmvc.xml。 DispatcherServlet 是前端控制器设计模式的实现，对于前段控制对应的就是 Spring MVC。
springmvc.xml springmvc.xml 的配置是在 web.xml 中的 servlet 标签中的 init-param 中设置。这里可以更改 xml 的位置和名称。我记得如果不配，默认是会在 WEB-INF 中创建一个 DispatcherServlet 的文件。现在我指定在了 resources 目录里面，将 Spring 的配置都放在一起。
首先都是 Spring MVC 的头，
&amp;lt;?xml version=&amp;#34;1.0&amp;#34; encoding=&amp;#34;UTF-8&amp;#34;?&amp;gt; &amp;lt;beans xmlns=&amp;#34;http://www.springframework.org/schema/beans&amp;#34; xmlns:xsi=&amp;#34;http://www.w3.org/2001/XMLSchema-instance&amp;#34; xmlns:context=&amp;#34;http://www.springframework.org/schema/context&amp;#34; xmlns:mvc=&amp;#34;http://www.springframework.org/schema/mvc&amp;#34; xsi:schemaLocation=&amp;#34;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc.xsd&amp;#34; default-lazy-init=&amp;#34;false&amp;#34;&amp;gt; 这里还是要注意这个 default-lazy-init=&amp;quot;false&amp;quot; 这句。
配置外在应用参数。有些参数写在xml虽然可以，但是经常会更改，这些数据可以放到 properties 的文件里，比如用户名，管理员密码等。
&amp;lt;context:property-placeholder location=&amp;#34;classpath:properties/resource.properties&amp;#34;/&amp;gt; 配组件扫描。由于这个项目使用的是注解形式搭建的，所以需要组件扫描器扫描特定包中的注解。这里要说明的是，因为 Spring MVC 主要面向的是前段控制层，所以 Spring MVC 扫描的是 Controller 包的注解。
&amp;lt;context:component-scan base-package=&amp;#34;com.summer.controller&amp;#34;/&amp;gt; 接下来配置的是注解驱动。这个还是蛮重要的，如果不配置，导致注解不能被解析，@RequestMapping 不能使用。</description>
    </item>
    
    <item>
      <title>Spring Configuration - web.xml</title>
      <link>http://keltoy.github.io/posts/spring-configuration-web-xml/</link>
      <pubDate>Tue, 18 Oct 2016 23:46:34 +0000</pubDate>
      
      <guid>http://keltoy.github.io/posts/spring-configuration-web-xml/</guid>
      <description>前言 关于 Spring 这个框架无需多说了，很经典了，这里先复习一下它的配置。话说不复习还真容易忘了。
Spring 配置 主要还是通过学习的项目，对 Spring, Spring MVC 和 Mybatis 这三大框架的配置进行复习，主要对 Spring的框架配置进行复习。
web.xml 此文件的目录是 src/main/webapp/WEB-INF/，该目录下还有 css, js, jsp 这三个目录。
&amp;lt;!DOCTYPE web-app PUBLIC &amp;#34;-//Sun Microsystems, Inc.//DTD Web Application 2.3//EN&amp;#34; &amp;#34;http://java.sun.com/dtd/web-app_2_3.dtd&amp;#34; &amp;gt; &amp;lt;web-app xmlns:xsi=&amp;#34;http://www.w3.org/2001/XMLSchema-instance&amp;#34; xmlns=&amp;#34;http://java.sun.com/xml/ns/javaee&amp;#34; xsi:schemaLocation=&amp;#34;http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_2_5.xsd&amp;#34; id=&amp;#34;WebApp_ID&amp;#34; version=&amp;#34;2.5&amp;#34;&amp;gt; 以上内容基本不怎么更改，使用默认的就行。 设置项目名称：
&amp;lt;display-name&amp;gt;summer&amp;lt;/display-name&amp;gt; 定制欢迎页，就是设置首页方式，首次访问的时候就会跳转到 welcome-file 设置的文件中。
&amp;lt;welcome-file-list&amp;gt; &amp;lt;welcome-file&amp;gt;index.html&amp;lt;/welcome-file&amp;gt; &amp;lt;welcome-file&amp;gt;index.htm&amp;lt;/welcome-file&amp;gt; &amp;lt;welcome-file&amp;gt;index.jsp&amp;lt;/welcome-file&amp;gt; &amp;lt;welcome-file&amp;gt;default.html&amp;lt;/welcome-file&amp;gt; &amp;lt;welcome-file&amp;gt;default.htm&amp;lt;/welcome-file&amp;gt; &amp;lt;welcome-file&amp;gt;default.jsp&amp;lt;/welcome-file&amp;gt; &amp;lt;/welcome-file-list&amp;gt; context-param 用来声明应用范围(整个WEB项目)内的上下文初始化参数。个人理解就是加载 Spring 配置文件，初始化 Spring 容器。 param-name 设定上下文的参数名称。必须是唯一名称。 param-value 设定的参数名称的值，可以设置为目录文件。
&amp;lt;context-param&amp;gt; &amp;lt;param-name&amp;gt;contextConfigLocation&amp;lt;/param-name&amp;gt; &amp;lt;param-value&amp;gt;classpath:spring/applicationContext*.xml&amp;lt;/param-value&amp;gt; &amp;lt;/context-param&amp;gt; 设置监听器。引用他人的介绍：
 ContextLoaderListener的作用就是启动Web容器时，自动装配ApplicationContext的配置信息。 因为它实现了ServletContextListener这个接口，在web.</description>
    </item>
    
    <item>
      <title>Class Loaders in Java</title>
      <link>http://keltoy.github.io/posts/class-loaders-in-java/</link>
      <pubDate>Thu, 13 Oct 2016 12:13:07 +0000</pubDate>
      
      <guid>http://keltoy.github.io/posts/class-loaders-in-java/</guid>
      <description>前言 好像一直没搞明白，但是每次都会被问到，总结总结，看看自己的理解到哪一步了。
初识 Class Loaders 首先，类加载器的架构：
+-----------------+ | | | +-------------+ | +--------------------------------+ | | Bootstrap | | &amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt; | Load JRE/lib/rt.jar | | | ClassLoader | | &amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt; | specified by -Xbootclasspath | | +-------------+ | +--------------------------------+ | | | +-------------+ | +--------------------------------+ | | Extension | | &amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt; | Load JRE/lib/ext/*.jar | | | ClassLoader | | &amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt; | specified by -Djava.ext.dirs | | +-------------+ | +--------------------------------+ | | | +-------------+ | +--------------------------------+ | | App | | &amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt; | Load CLASSPATH | | | ClassLoader | | &amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt; | specified by -Djava.</description>
    </item>
    
    <item>
      <title>Source Code in Java -- Integer</title>
      <link>http://keltoy.github.io/posts/source-code-in-java-integer/</link>
      <pubDate>Tue, 11 Oct 2016 00:41:30 +0000</pubDate>
      
      <guid>http://keltoy.github.io/posts/source-code-in-java-integer/</guid>
      <description>前言  Most of you are familiar with the virtues of a programmer. There are three, of course: laziness, impatience, and hubris. &amp;ndash; Larry Wall
 细细数来，我好像还没有做过源码的东西，不看看优秀的源码，如何才能够有长远的进步呢？就从 Java 开始，边看边总结这些源码。先看Integer。
初识 Integer 首先Integer是一个类，包装 int 的类，为了更好地和其他方法和范型配合，所以需要把基础类型包装成一个类。既然是类当然就可以设置为 null，这是基本类型做不到的，基本类型初始化也只能是 0。 Integer 和 int 在实际操作过程中是可以相等的，Integer 在匹配 int 类型的数据的时候，就会自动装箱和拆箱。 基本上，我对 Integer 的理解也就到这个程度。这样就可以理解为什么以下的结果有所不同：
int i = 0; Integer i2 = 0; Integer i3 = new Integer(0); Integer i4 = new Integer(0); Integer i5 = 0; System.out.println(i == i2); // true System.</description>
    </item>
    
    <item>
      <title>How to display grid in Pandas</title>
      <link>http://keltoy.github.io/posts/how-to-display-grid-in-pandas/</link>
      <pubDate>Sat, 08 Oct 2016 17:07:57 +0000</pubDate>
      
      <guid>http://keltoy.github.io/posts/how-to-display-grid-in-pandas/</guid>
      <description>前言  “Life is short, You need Python.” &amp;ndash; Bruce Eckel
 Manager说是要把处理的数据展示出来&amp;hellip;&amp;hellip;我就想偷个懒直接使用Matplotlib实现得了，省着使用 D3.js 了，当然如果需要的话，还是很好转过来的。
首先吧，因为不仅需要展示，还需要处理后的数据，所以为了其他人方便，就吧结果存成了csv格式的了。其他格式都大同小异，js还在学习过程中，而且公司的本调试起来非常不方便，所以才使用的 Python&amp;hellip;
使用DataFrame展示 一开始想的就是调用 matplotlib.pylab 画图，表里面的一个 column 作为横轴，另一个column作为纵轴然后就可以了。但是后来我发现没那么复杂，DataFrame 直接就可以调用 plot&amp;hellip;.
import pandas as import pd df = pd.read_csv(filename) df.plot() 发现了这个之后感觉我的代码又一次简化了&amp;hellip;.
显示网格 这个其实没啥说的， 看一下 plot 的 api 有一个参数就叫做 grid，把这个赋值成 True，就行了。
df.plot(grid=True) 显示点 然后我就给了我的Manager。Manager说，不行啊，图上的点都没有突出&amp;hellip; 于是图形再次修改，查到 plot 函数是实现 matplotlib 的 plotting 方法的，所以可以使用 marker 参数的，于是又有了如下修改。
df.plot(marker=&amp;#39;o&amp;#39;, grid=True) 其中 &amp;lsquo;o&amp;rsquo; 代表的就是每个点使用圆圈标注。
调整刻度 这下应该可以了吧？ Manager 又来传话：网格的间距太大，能不能缩小一点？ 呃&amp;hellip;. 我看了看 API，貌似没有什么可以把网格缩小的方法&amp;hellip;. 于是，我又一次把问题想复杂了&amp;hellip;. 网格的大小实际上就是刻度的大小，如果刻度数量太多了，那么 DataFrame 自己会进行调整，但是这样的调整可能太大了，不符合人眼观测。所以，调整轴间距就可以完全满足。xticks 和 yticks，这里我只使用了xticks。调整完毕之后发现一连串的问题，生成的图片太小，根本看不清楚，横轴的标识全都挤在了一起无法辨认，很临界点看着不舒服&amp;hellip;.</description>
    </item>
    
    <item>
      <title>Mathematics for Thinking Code</title>
      <link>http://keltoy.github.io/posts/mathematics-for-thinking-code/</link>
      <pubDate>Sun, 02 Oct 2016 14:42:21 +0000</pubDate>
      
      <guid>http://keltoy.github.io/posts/mathematics-for-thinking-code/</guid>
      <description>Preface  Histories make men wise; poets, witty; the mathematics, subtle; natural philosophy, deep; moral, grave; logic and rhetoric, able to contend. &amp;ndash; Francis Bacon, The Collected Works of Sir Francis Bacon
 It is very significant that mathematics for programmer. A skilled programmer, who is good at mathematics, is able to simplify problems whatever in life or engineering. A textbook, Mathematics for Computer Science, makes me learn a lot.</description>
    </item>
    
    <item>
      <title>时间复杂度</title>
      <link>http://keltoy.github.io/posts/%E6%97%B6%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6/</link>
      <pubDate>Fri, 30 Sep 2016 11:45:33 +0000</pubDate>
      
      <guid>http://keltoy.github.io/posts/%E6%97%B6%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6/</guid>
      <description>所有的时间复杂度 其实是这个网站的图。。。感觉好用就拿来了。 还有一个是各类时间复杂度的比较。
   数据结构 时间复杂度 &amp;ndash; &amp;ndash; &amp;ndash; &amp;ndash; &amp;ndash; &amp;ndash; &amp;ndash; 空间复杂度      平均 &amp;ndash; &amp;ndash; &amp;ndash; 最快 &amp;ndash; &amp;ndash; &amp;ndash; 最坏    访问 查找 插入 删除 访问 查找 插入 删除    Array $$\theta(1)$$ $$\theta(n)$$ $$\theta(n)$$ $$\theta(n)$$ $$O(1)$$ $$O(n)$$ $$O(n)$$ $$O(n)$$ $$O(n)$$   Stack $$\theta(n)$$ $$\theta(n)$$ $$\theta(1)$$ $$\theta(1)$$ $$O(n)$$ $$O(n)$$ $$O(1)$$ $$O(1)$$ $$O(n)$$   Queue $$\theta(n)$$ $$\theta(n)$$ $$\theta(1)$$ $$\theta(1)$$ $$O(n)$$ $$O(n)$$ $$O(1)$$ $$O(1)$$ $$O(n)$$   Singly-Linked List $$\theta(n)$$ $$\theta(n)$$ $$\theta(1)$$ $$\theta(1)$$ $$O(n)$$ $$O(n)$$ $$O(1)$$ $$O(1)$$ $$O(n)$$   Doubly-Linked List $$\theta(n)$$ $$\theta(n)$$ $$\theta(1)$$ $$\theta(1)$$ $$O(n)$$ $$O(n)$$ $$O(1)$$ $$O(1)$$ $$O(n)$$   Skip List $$\theta(\log_2(n))$$ $$\theta(\log_2(n))$$ $$\theta(\log_2(n))$$ $$\theta(\log_2(n))$$ $$O(n)$$ $$O(n)$$ $$O(n)$$ $$O(n)$$ $$O(n\log_2(n))$$   Hash Table N/A $$\theta(1)$$ $$\theta(1)$$ $$\theta(1)$$ N/A $$O(n)$$ $$O(n)$$ $$O(n)$$ $$O(n)$$   Binary Search Tree $$\theta(\log_2(n))$$ $$\theta(\log_2(n))$$ $$\theta(\log_2(n))$$ $$\theta(\log_2(n))$$ $$O(n)$$ $$O(n)$$ $$O(n)$$ $$O(n)$$ $$O(n)$$   Cartesian Tree N/A $$\theta(\log_2(n))$$ $$\theta(\log_2(n))$$ $$\theta(\log_2(n))$$ N/A $$O(n)$$ $$O(n)$$ $$O(n)$$ $$O(n)$$   B-Tree $$\theta(\log_2(n))$$ $$\theta(\log_2(n))$$ $$\theta(\log_2(n))$$ $$\theta(\log_2(n))$$ $$O(\log_2(n))$$ $$O(\log_2(n))$$ $$O(\log_2(n))$$ $$O(\log_2(n))$$ $$O(n)$$   Red-Black Tree $$\theta(\log_2(n))$$ $$\theta(\log_2(n))$$ $$\theta(\log_2(n))$$ $$\theta(\log_2(n))$$ $$O(\log_2(n))$$ $$O(\log_2(n))$$ $$O(\log_2(n))$$ $$O(\log_2(n))$$ $$O(n)$$   Splay Tree N/A $$\theta(\log_2(n))$$ $$\theta(\log_2(n))$$ $$\theta(\log_2(n))$$ N/A $$O(\log_2(n))$$ $$O(\log_2(n))$$ $$O(\log_2(n))$$ $$O(n)$$   AVL Tree $$\theta(\log_2(n))$$ $$\theta(\log_2(n))$$ $$\theta(\log_2(n))$$ $$\theta(\log_2(n))$$ $$O(\log_2(n))$$ $$O(\log_2(n))$$ $$O(\log_2(n))$$ $$O(\log_2(n))$$ $$O(n)$$   KD Tree $$\theta(\log_2(n))$$ $$\theta(\log_2(n))$$ $$\theta(\log_2(n))$$ $$\theta(\log_2(n))$$ $$O(n)$$ $$O(n)$$ $$O(n)$$ $$O(n)$$ $$O(n)$$    还有排序时间复杂度：</description>
    </item>
    
    <item>
      <title>编程中的临界值</title>
      <link>http://keltoy.github.io/posts/%E7%BC%96%E7%A8%8B%E4%B8%AD%E7%9A%84%E4%B8%B4%E7%95%8C%E5%80%BC/</link>
      <pubDate>Sun, 25 Sep 2016 12:51:55 +0000</pubDate>
      
      <guid>http://keltoy.github.io/posts/%E7%BC%96%E7%A8%8B%E4%B8%AD%E7%9A%84%E4%B8%B4%E7%95%8C%E5%80%BC/</guid>
      <description>前言 刷题、面试、OJ 过程中，总是被编程题所困扰，往往是有思路能解答，但是不能 100% AC，看着刷子和cheater们拿走offer，心里有种说不出的感觉，内心下定决心，一定要完成 100% AC &amp;hellip;
于是我稳稳心态，吃根辣条压压惊，刷刷OJ，找找感觉，在这里总结一下如何能够考虑到所有的情况。
不能 100% AC 顾名思义，基本上我所出现的问题都是，思路正确，编程问题不大，但是不够严谨，很多时候不能够考虑到所有的情况，导致 AC 的通过率总是在 60%~90% 徘徊。如何才能够考虑到所有情况，才是我所要解决的关键。
通常情况 通常情况一般就是 OJ 里面所举例说明的案例，这个案例如果过不了则说明所选的算法可能本身就有问题。不论是 hash，backtrack，dp，recursion 还是其他，首先考虑所选用的方法是否合适解决此类问题。如果能够通过说明案例，至少说明方法应该不会错太多。
临界值 临界值这个首先要看清楚题目所给出的范围，比如说： $$0 &amp;lt;= m &amp;lt;= 10^5$$ 这里就要注意两边注意两边的值一个是可以取到 0 的， 而另一边，要看看是否大于了int类型的临界值 (Integer.MAX_VALUE()) 等等。 输出也是需要有临界值的，所以输出也是需要判断的。 还有一类就是自己加的一些参数，这些参数也可能影响结果&amp;hellip; 比如说，给定的输入只有一个n，但是我要的方法需要 recursion，需要在方法中添加一个 res, 和 ix，这时候需要注意这两个值有没有临界值。
void solution(int n, int res, int ix) { if (res &amp;gt; SOMEVALUE) { // my code...  } if (ix &amp;gt; SOMEINDEX) { // my code...  } // my code.</description>
    </item>
    
    <item>
      <title>Hello World</title>
      <link>http://keltoy.github.io/posts/hello-world/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://keltoy.github.io/posts/hello-world/</guid>
      <description>Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.
Quick Start Create a new post $ hexo new &amp;#34;My New Post&amp;#34; More info: Writing
Run server $ hexo server More info: Server
Generate static files $ hexo generate More info: Generating
Deploy to remote sites $ hexo deploy More info: Deployment</description>
    </item>
    
  </channel>
</rss>
