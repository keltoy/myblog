---
title: 大数据框架
date: 2020-07-01 10:40:25
tags: [ data ]
---

# 大数据框架

[toc]

## 背景

做数据处理不能只是出报表，有很多东西还需要去处理，比如用户画像，比如数据分析，比如推荐系统，比如数据仓库。

现在处理数据不像原来，只需要处理离线数据，这样报表也只能出T-1天的数据，很多时候，我们希望能看到实时数据。这样就会出现一些问题：

更新频率，一天、一小时更新，这样可以不使用实时处理，可能体现不了实时的意义；如果都改成流数据处理，更新频率可以到达分钟，秒级，但是数据量不全，缺少历史数据，可能不能保证准确性。因此需要设计一种架构技能满足实时处理，又要保证历史数据准确。

## Lambda 框架

Lambda架构的设计为了处理大规模数据时，同时发挥流数据处理和批处理的优势。通过离线批处理提供全面、准确的数据；通过实时流处理提供低延迟的数据，达到平衡延迟、吞吐量和容错性的目的。

Lambda架构包含3层Batch Layer， Speed Layer，Serving Layer

![lambda](lambda.jpg)

- Batch Layer: 批处理层，对离线的历史数据进行预结算，为了下游能够快速查询想要的结果。由于批处理基于完整的历史数据集因此准确性可以得到保证。批处理可以用Hadoop/Spark/Flink等框架计算
- Speed Layer: 加速处理层，处理实时的增量数据，这一层重点在于低延迟。加速层的数据不如批处理那样完整和准确，但是可以填补批处理高延迟导致的数据的空白。加速层可以使用 Storm/Spark streaming/Flink等计算框架
- Serving Layer: 合并服务层 合并层将批处理和加速层的的数据合并，输出出来或者提供给下游来分析

![v2-22ac450d4620a9f7e644d5bb3c065774_720w](v2-22ac450d4620a9f7e644d5bb3c065774_720w.jpg)

IBM 使用的一套Lambda

Lambda 的出现，很好地解决了离线与实时处理二者都能发挥出功效，离线批处理 和 实时数据 都体现了各自的优势，晚上可以跑离线任务，而实时任务一般也是集中在白天，让实时成本可控，且错开了高峰时间

不过随着时代的发展，Lambda 面对当前复杂的业务分析需求逐渐力不从心，暴露出了以下几个问题：

1. 实时与批量计算结果不一致引起的数据口径不一致
2. 批量计算在晚上计算窗口内无法完成
3. 开发和维护复杂，烟囱式开发没份数据需要至少处理2次
4. 服务器内存大

## Kappa框架

[Kappa架构](http://milinda.pathirage.org/kappa-architecture.com/) 简化了[Lambda架构](http://lambda-architecture.net/)。[Kappa架构](http://milinda.pathirage.org/kappa-architecture.com/)系统是删除了批处理系统的架构。要取代批处理，数据只需通过流式传输系统快速提供：

![20190702150038971](20190702150038971.png)

那如何用流计算系统对全量数据进行重新计算，步骤如下：

1. 用Kafka或类似的分布式队列保存数据，需要几天数据量就保存几天。

2. 当需要全量计算时，重新起一个流计算实例，从头开始读取数据进行处理，并输出到一个结果存储中。

3. 当新的实例完成后，停止老的流计算实例，并把老的一引起结果删除。

和Lambda架构相比，在Kappa架构下，只有在有必要的时候才会对历史数据进行重复计算，并且实时计算和批处理过程使用的是同一份代码。或许有些人会质疑流式处理对于历史数据的高吞吐量会力不从心，但是这可以通过控制新实例的并发数进行改善。

Kappa架构的核心思想包括以下三点：

- 用Kafka或者类似的分布式队列系统保存数据，你需要几天的数据量就保存几天。

- 当需要全量重新计算时，重新起一个流计算实例，从头开始读取数据进行处理，并输出到一个新的结果存储中。

- 当新的实例做完后，停止老的流计算实例，并把老的一些结果删除。

## Iota 框架

在IOT大潮下，智能手机、PC、智能硬件设备的计算能力越来越强，而业务需求要求数据实时响应需求能力也越来越强，过去传统的中心化、非实时化数据处理的思路已经不适应现在的大数据分析需求，我提出新一代的大数据IOTA架构来解决上述问题，整体思路是设定标准数据模型，通过边缘计算技术把所有的计算过程分散在数据产生、计算和查询过程当中，以统一的数据模型贯穿始终，从而提高整体的预算效率，同时满足即时计算的需要，可以使用各种Ad-hoc Query来查询底层数据

## ![46858-a76c62b91f0f40a9](46858-a76c62b91f0f40a9.png)

IOTA整体技术结构分为几部分：

● Common Data Model：贯穿整体业务始终的数据模型，这个模型是整个业务的核心，要保持SDK、cache、历史数据、查询引擎保持一致。对于用户数据分析来讲可以定义为“主-谓-宾”或者“对象-事件”这样的抽象模型来满足各种各样的查询。以大家熟悉的APP用户模型为例，用“主-谓-宾”模型描述就是“X用户 – 事件1 – A页面（2018/4/11 20:00） ”。当然，根据业务需求的不同，也可以使用“产品-事件”、“地点-时间”模型等等。模型本身也可以根据协议（例如 protobuf）来实现SDK端定义，中央存储的方式。此处核心是，从SDK到存储到处理是统一的一个Common Data Model。

● Edge SDKs & Edge Servers：这是数据的采集端，不仅仅是过去的简单的SDK，在复杂的计算情况下，会赋予SDK更复杂的计算，在设备端就转化为形成统一的数据模型来进行传送。例如对于智能Wi-Fi采集的数据，从AC端就变为“X用户的MAC 地址-出现- A楼层（2018/4/11 18:00）”这种主-谓-宾结构，对于摄像头会通过Edge AI Server，转化成为“X的Face特征- 进入- A火车站（2018/4/11 20:00）”。也可以是上面提到的简单的APP或者页面级别的“X用户 – 事件1 – A页面（2018/4/11 20:00） ”，对于APP和H5页面来讲，没有计算工作量，只要求埋点格式即可。

● Real Time Data：实时数据缓存区，这部分是为了达到实时计算的目的，海量数据接收不可能海量实时入历史数据库，那样会出现建立索引延迟、历史数据碎片文件等问题。因此，有一个实时数据缓存区来存储最近几分钟或者几秒钟的数据。这块可以使用Kudu或者Hbase等组件来实现。这部分数据会通过Dumper来合并到历史数据当中。此处的数据模型和SDK端数据模型是保持一致的，都是Common Data Model，例如“主-谓-宾”模型。

● Historical Data：历史数据沉浸区，这部分是保存了大量的历史数据，为了实现Ad-hoc查询，将自动建立相关索引提高整体历史数据查询效率，从而实现秒级复杂查询百亿条数据的反馈。例如可以使用HDFS存储历史数据，此处的数据模型依然SDK端数据模型是保持一致的Common Data Model。

● Dumper：Dumper的主要工作就是把最近几秒或者几分钟的实时数据，根据汇聚规则、建立索引，存储到历史存储结构当中，可以使用map reduce、C、Scala来撰写，把相关的数据从Realtime Data区写入Historical Data区。

● Query Engine：查询引擎，提供统一的对外查询接口和协议（例如SQL JDBC），把Realtime Data和Historical Data合并到一起查询，从而实现对于数据实时的Ad-hoc查询。例如常见的计算引擎可以使用presto、impala、clickhouse等。

● Realtime model feedback：通过Edge computing技术，在边缘端有更多的交互可以做，可以通过在Realtime Data去设定规则来对Edge SDK端进行控制，例如，数据上传的频次降低、语音控制的迅速反馈，某些条件和规则的触发等等。简单的事件处理，将通过本地的IOT端完成，例如，嫌疑犯的识别现在已经有很多摄像头本身带有此功能。

IOTA大数据架构，主要有如下几个特点：

● 去ETL化：ETL和相关开发一直是大数据处理的痛点，IOTA架构通过Common Data Model的设计，专注在某一个具体领域的数据计算，从而可以从SDK端开始计算，中央端只做采集、建立索引和查询，提高整体数据分析的效率。

● Ad-hoc即时查询：鉴于整体的计算流程机制，在手机端、智能IOT事件发生之时，就可以直接传送到云端进入real time data区，可以被前端的Query Engine来查询。此时用户可以使用各种各样的查询，直接查到前几秒发生的事件，而不用在等待ETL或者Streaming的数据研发和处理。

● 边缘计算（Edge-Computing）：将过去统一到中央进行整体计算，分散到数据产生、存储和查询端，数据产生既符合Common Data Model。同时，也给与Realtime model feedback，让客户端传送数据的同时马上进行反馈，而不需要所有事件都要到中央端处理之后再进行下发。

## 总结

这就是网上比较常见的3中框架对于目前的工作，可能不能实现其全部，但是这样的一个演进过程，是值得我们学习和指引的

